{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hg8LXAe8mt-G"
   },
   "source": [
    "# **VideoCrafter：A Toolkit for Text-to-Video Generation and Editing**\n",
    "\n",
    "\n",
    "VideoCrafter is an open-source video generation and editing toolbox for crafting video content.\n",
    "\n",
    "More details can be founded in [![GitHub](https://img.shields.io/github/stars/VideoCrafter/VideoCrafter?style=social)](https://github.com/VideoCrafter/VideoCrafter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijIJmBFLebe3",
    "outputId": "4a54fb53-cf48-4e1b-9a2f-a667ffefbe4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090 Ti, 24564 MiB, 24233 MiB\r\n"
     ]
    }
   ],
   "source": [
    "### make sure that CUDA is available in Edit -> Nootbook settings -> GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOH7mJYFl8Y0"
   },
   "source": [
    "### Base T2V: Generic Text-to-video Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/src/notebooks/T2V_models/VideoCrafter/VideoCrafter\n",
      "Obtaining file:///src/notebooks/T2V_models/VideoCrafter/VideoCrafter\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hInstalling collected packages: lvdm\n",
      "  Attempting uninstall: lvdm\n",
      "    Found existing installation: lvdm 0.0.1\n",
      "    Can't uninstall 'lvdm'. No files were found to uninstall.\n",
      "  Running setup.py develop for lvdm\n",
      "Successfully installed lvdm-0.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%cd VideoCrafter\n",
    "!python3.8 -m pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0djMBodhGw7",
    "outputId": "91c0a4d1-1cfb-42ba-c79b-af38c8adc071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'VideoCrafter'\n",
      "/src/notebooks/T2V_models/VideoCrafter/VideoCrafter\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH=\"models/base_t2v/model.ckpt\"\n",
    "CONFIG_PATH=\"models/base_t2v/model_config.yaml\"\n",
    "%cd VideoCrafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def read_in_file(filename:str) -> list:\n",
    "    \"\"\"\n",
    "    This functions reads in a text file containing the text prompts,\n",
    "    this prompts will be used as the input for the T2V model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        input_file= open(file=filename, mode=\"r\", encoding=\"utf-8\") \n",
    "        sentences = input_file.read()\n",
    "        sentences= sentences.split(\"\\n\")\n",
    "        input_file.close()\n",
    "        return sentences\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file {filename} doesn't exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "text_prompts = read_in_file(\"../../../generated_videos/prompts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0djMBodhGw7",
    "outputId": "91c0a4d1-1cfb-42ba-c79b-af38c8adc071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-20 17:00:31.230651: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-20 17:00:31.265646: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 17:00:31.796125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Global seed set to 1000\n",
      "config: \n",
      " {'model': {'target': 'lvdm.models.ddpm3d.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'video', 'cond_stage_key': 'caption', 'image_size': [32, 32], 'video_length': 16, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'scale_by_std': False, 'scale_factor': 0.18215, 'unet_config': {'target': 'lvdm.models.modules.openaimodel3d.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False, 'kernel_size_t': 1, 'padding_t': 0, 'temporal_length': 16, 'use_relative_position': True}}, 'first_stage_config': {'target': 'lvdm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'lvdm.models.modules.condition_modules.FrozenCLIPEmbedder'}}}}\n",
      "Loading model from models/base_t2v/model.ckpt\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92m<module>\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m# --------------------------------------------------------------------\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m263 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m264 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m202\u001b[0m in \u001b[92mmain\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mconfig: \u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, config)                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# get model & sampler\u001b[0m                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m202 \u001b[2m│   \u001b[0mmodel, _, _ = load_model(config, opt.ckpt_path,                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0minject_lora=opt.inject_lora,              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_scale=opt.lora_scale,                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m205 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_path=opt.lora_path                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/../scripts/\u001b[0m\u001b[1;33msampl\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33me_utils.py\u001b[0m:\u001b[94m14\u001b[0m in \u001b[92mload_model\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 11 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading model from \u001b[0m\u001b[33m{\u001b[0mckpt_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 12 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# load sd\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 14 \u001b[2m│   \u001b[0mpl_sd = torch.load(ckpt_path, map_location=\u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m)                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[2m│   │   \u001b[0mglobal_step = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mglobal_step\u001b[0m\u001b[33m\"\u001b[0m]                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m\u001b[2m│   │   \u001b[0mepoch = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mepoch\u001b[0m\u001b[33m\"\u001b[0m]                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m713\u001b[0m in \u001b[92mload\u001b[0m    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 710 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mopened_file.seek(orig_position)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.jit.load(opened_file)                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, pickle_mod \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 713 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _legacy_load(opened_file, map_location, pickle_module, \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 714 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 715 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m# Register pickling support for layout instances such as\u001b[0m              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m920\u001b[0m in         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[92m_legacy_load\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 917 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mReceived object of type \u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(f)\u001b[33m}\u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m. Please update to\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 918 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mfunctionality.\u001b[0m\u001b[33m\"\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 919 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 920 \u001b[2m│   \u001b[0mmagic_number = pickle_module.load(f, **pickle_load_args)          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 921 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m magic_number != MAGIC_NUMBER:                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInvalid magic number; corrupt file?\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   \u001b[0mprotocol_version = pickle_module.load(f, **pickle_load_args)      \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mUnpicklingError: \u001b[0minvalid load key, \u001b[32m'v'\u001b[0m.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-20 17:00:34.820914: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-20 17:00:34.855942: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 17:00:35.377996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Global seed set to 1000\n",
      "config: \n",
      " {'model': {'target': 'lvdm.models.ddpm3d.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'video', 'cond_stage_key': 'caption', 'image_size': [32, 32], 'video_length': 16, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'scale_by_std': False, 'scale_factor': 0.18215, 'unet_config': {'target': 'lvdm.models.modules.openaimodel3d.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False, 'kernel_size_t': 1, 'padding_t': 0, 'temporal_length': 16, 'use_relative_position': True}}, 'first_stage_config': {'target': 'lvdm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'lvdm.models.modules.condition_modules.FrozenCLIPEmbedder'}}}}\n",
      "Loading model from models/base_t2v/model.ckpt\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92m<module>\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m# --------------------------------------------------------------------\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m263 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m264 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m202\u001b[0m in \u001b[92mmain\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mconfig: \u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, config)                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# get model & sampler\u001b[0m                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m202 \u001b[2m│   \u001b[0mmodel, _, _ = load_model(config, opt.ckpt_path,                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0minject_lora=opt.inject_lora,              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_scale=opt.lora_scale,                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m205 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_path=opt.lora_path                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/../scripts/\u001b[0m\u001b[1;33msampl\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33me_utils.py\u001b[0m:\u001b[94m14\u001b[0m in \u001b[92mload_model\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 11 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading model from \u001b[0m\u001b[33m{\u001b[0mckpt_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 12 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# load sd\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 14 \u001b[2m│   \u001b[0mpl_sd = torch.load(ckpt_path, map_location=\u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m)                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[2m│   │   \u001b[0mglobal_step = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mglobal_step\u001b[0m\u001b[33m\"\u001b[0m]                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m\u001b[2m│   │   \u001b[0mepoch = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mepoch\u001b[0m\u001b[33m\"\u001b[0m]                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m713\u001b[0m in \u001b[92mload\u001b[0m    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 710 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mopened_file.seek(orig_position)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.jit.load(opened_file)                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, pickle_mod \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 713 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _legacy_load(opened_file, map_location, pickle_module, \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 714 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 715 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m# Register pickling support for layout instances such as\u001b[0m              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m920\u001b[0m in         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[92m_legacy_load\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 917 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mReceived object of type \u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(f)\u001b[33m}\u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m. Please update to\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 918 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mfunctionality.\u001b[0m\u001b[33m\"\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 919 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 920 \u001b[2m│   \u001b[0mmagic_number = pickle_module.load(f, **pickle_load_args)          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 921 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m magic_number != MAGIC_NUMBER:                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInvalid magic number; corrupt file?\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   \u001b[0mprotocol_version = pickle_module.load(f, **pickle_load_args)      \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mUnpicklingError: \u001b[0minvalid load key, \u001b[32m'v'\u001b[0m.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-20 17:00:38.422944: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-20 17:00:38.458654: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 17:00:38.983258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Global seed set to 1000\n",
      "config: \n",
      " {'model': {'target': 'lvdm.models.ddpm3d.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'video', 'cond_stage_key': 'caption', 'image_size': [32, 32], 'video_length': 16, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'scale_by_std': False, 'scale_factor': 0.18215, 'unet_config': {'target': 'lvdm.models.modules.openaimodel3d.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False, 'kernel_size_t': 1, 'padding_t': 0, 'temporal_length': 16, 'use_relative_position': True}}, 'first_stage_config': {'target': 'lvdm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'lvdm.models.modules.condition_modules.FrozenCLIPEmbedder'}}}}\n",
      "Loading model from models/base_t2v/model.ckpt\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92m<module>\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m# --------------------------------------------------------------------\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m263 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m264 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m202\u001b[0m in \u001b[92mmain\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mconfig: \u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, config)                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# get model & sampler\u001b[0m                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m202 \u001b[2m│   \u001b[0mmodel, _, _ = load_model(config, opt.ckpt_path,                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0minject_lora=opt.inject_lora,              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_scale=opt.lora_scale,                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m205 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_path=opt.lora_path                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/../scripts/\u001b[0m\u001b[1;33msampl\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33me_utils.py\u001b[0m:\u001b[94m14\u001b[0m in \u001b[92mload_model\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 11 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading model from \u001b[0m\u001b[33m{\u001b[0mckpt_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 12 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# load sd\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 14 \u001b[2m│   \u001b[0mpl_sd = torch.load(ckpt_path, map_location=\u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m)                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[2m│   │   \u001b[0mglobal_step = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mglobal_step\u001b[0m\u001b[33m\"\u001b[0m]                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m\u001b[2m│   │   \u001b[0mepoch = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mepoch\u001b[0m\u001b[33m\"\u001b[0m]                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m713\u001b[0m in \u001b[92mload\u001b[0m    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 710 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mopened_file.seek(orig_position)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.jit.load(opened_file)                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, pickle_mod \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 713 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _legacy_load(opened_file, map_location, pickle_module, \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 714 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 715 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m# Register pickling support for layout instances such as\u001b[0m              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m920\u001b[0m in         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[92m_legacy_load\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 917 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mReceived object of type \u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(f)\u001b[33m}\u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m. Please update to\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 918 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mfunctionality.\u001b[0m\u001b[33m\"\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 919 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 920 \u001b[2m│   \u001b[0mmagic_number = pickle_module.load(f, **pickle_load_args)          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 921 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m magic_number != MAGIC_NUMBER:                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInvalid magic number; corrupt file?\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   \u001b[0mprotocol_version = pickle_module.load(f, **pickle_load_args)      \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mUnpicklingError: \u001b[0minvalid load key, \u001b[32m'v'\u001b[0m.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-20 17:00:42.049586: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-20 17:00:42.084347: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 17:00:42.604523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Global seed set to 1000\n",
      "config: \n",
      " {'model': {'target': 'lvdm.models.ddpm3d.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'video', 'cond_stage_key': 'caption', 'image_size': [32, 32], 'video_length': 16, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'scale_by_std': False, 'scale_factor': 0.18215, 'unet_config': {'target': 'lvdm.models.modules.openaimodel3d.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False, 'kernel_size_t': 1, 'padding_t': 0, 'temporal_length': 16, 'use_relative_position': True}}, 'first_stage_config': {'target': 'lvdm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'lvdm.models.modules.condition_modules.FrozenCLIPEmbedder'}}}}\n",
      "Loading model from models/base_t2v/model.ckpt\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92m<module>\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m# --------------------------------------------------------------------\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m263 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m264 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m202\u001b[0m in \u001b[92mmain\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mconfig: \u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, config)                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# get model & sampler\u001b[0m                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m202 \u001b[2m│   \u001b[0mmodel, _, _ = load_model(config, opt.ckpt_path,                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0minject_lora=opt.inject_lora,              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_scale=opt.lora_scale,                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m205 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_path=opt.lora_path                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/../scripts/\u001b[0m\u001b[1;33msampl\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33me_utils.py\u001b[0m:\u001b[94m14\u001b[0m in \u001b[92mload_model\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 11 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading model from \u001b[0m\u001b[33m{\u001b[0mckpt_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 12 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# load sd\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 14 \u001b[2m│   \u001b[0mpl_sd = torch.load(ckpt_path, map_location=\u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m)                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[2m│   │   \u001b[0mglobal_step = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mglobal_step\u001b[0m\u001b[33m\"\u001b[0m]                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m\u001b[2m│   │   \u001b[0mepoch = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mepoch\u001b[0m\u001b[33m\"\u001b[0m]                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m713\u001b[0m in \u001b[92mload\u001b[0m    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 710 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mopened_file.seek(orig_position)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.jit.load(opened_file)                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, pickle_mod \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 713 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _legacy_load(opened_file, map_location, pickle_module, \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 714 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 715 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m# Register pickling support for layout instances such as\u001b[0m              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m920\u001b[0m in         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[92m_legacy_load\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 917 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mReceived object of type \u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(f)\u001b[33m}\u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m. Please update to\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 918 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mfunctionality.\u001b[0m\u001b[33m\"\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 919 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 920 \u001b[2m│   \u001b[0mmagic_number = pickle_module.load(f, **pickle_load_args)          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 921 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m magic_number != MAGIC_NUMBER:                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInvalid magic number; corrupt file?\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   \u001b[0mprotocol_version = pickle_module.load(f, **pickle_load_args)      \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mUnpicklingError: \u001b[0minvalid load key, \u001b[32m'v'\u001b[0m.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-20 17:00:45.668651: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-20 17:00:45.703545: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 17:00:46.233024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Global seed set to 1000\n",
      "config: \n",
      " {'model': {'target': 'lvdm.models.ddpm3d.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'video', 'cond_stage_key': 'caption', 'image_size': [32, 32], 'video_length': 16, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'scale_by_std': False, 'scale_factor': 0.18215, 'unet_config': {'target': 'lvdm.models.modules.openaimodel3d.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False, 'kernel_size_t': 1, 'padding_t': 0, 'temporal_length': 16, 'use_relative_position': True}}, 'first_stage_config': {'target': 'lvdm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'lvdm.models.modules.condition_modules.FrozenCLIPEmbedder'}}}}\n",
      "Loading model from models/base_t2v/model.ckpt\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92m<module>\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m# --------------------------------------------------------------------\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m263 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m264 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m202\u001b[0m in \u001b[92mmain\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mconfig: \u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, config)                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# get model & sampler\u001b[0m                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m202 \u001b[2m│   \u001b[0mmodel, _, _ = load_model(config, opt.ckpt_path,                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0minject_lora=opt.inject_lora,              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_scale=opt.lora_scale,                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m205 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_path=opt.lora_path                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/../scripts/\u001b[0m\u001b[1;33msampl\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33me_utils.py\u001b[0m:\u001b[94m14\u001b[0m in \u001b[92mload_model\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 11 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading model from \u001b[0m\u001b[33m{\u001b[0mckpt_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 12 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# load sd\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 14 \u001b[2m│   \u001b[0mpl_sd = torch.load(ckpt_path, map_location=\u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m)                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[2m│   │   \u001b[0mglobal_step = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mglobal_step\u001b[0m\u001b[33m\"\u001b[0m]                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m\u001b[2m│   │   \u001b[0mepoch = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mepoch\u001b[0m\u001b[33m\"\u001b[0m]                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m713\u001b[0m in \u001b[92mload\u001b[0m    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 710 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mopened_file.seek(orig_position)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.jit.load(opened_file)                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, pickle_mod \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 713 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _legacy_load(opened_file, map_location, pickle_module, \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 714 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 715 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m# Register pickling support for layout instances such as\u001b[0m              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m920\u001b[0m in         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[92m_legacy_load\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 917 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mReceived object of type \u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(f)\u001b[33m}\u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m. Please update to\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 918 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mfunctionality.\u001b[0m\u001b[33m\"\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 919 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 920 \u001b[2m│   \u001b[0mmagic_number = pickle_module.load(f, **pickle_load_args)          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 921 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m magic_number != MAGIC_NUMBER:                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInvalid magic number; corrupt file?\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   \u001b[0mprotocol_version = pickle_module.load(f, **pickle_load_args)      \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mUnpicklingError: \u001b[0minvalid load key, \u001b[32m'v'\u001b[0m.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-20 17:00:49.275044: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-20 17:00:49.310567: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 17:00:49.836910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Global seed set to 1000\n",
      "config: \n",
      " {'model': {'target': 'lvdm.models.ddpm3d.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'video', 'cond_stage_key': 'caption', 'image_size': [32, 32], 'video_length': 16, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'scale_by_std': False, 'scale_factor': 0.18215, 'unet_config': {'target': 'lvdm.models.modules.openaimodel3d.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False, 'kernel_size_t': 1, 'padding_t': 0, 'temporal_length': 16, 'use_relative_position': True}}, 'first_stage_config': {'target': 'lvdm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'lvdm.models.modules.condition_modules.FrozenCLIPEmbedder'}}}}\n",
      "Loading model from models/base_t2v/model.ckpt\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92m<module>\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m# --------------------------------------------------------------------\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m263 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m264 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m202\u001b[0m in \u001b[92mmain\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mconfig: \u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, config)                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# get model & sampler\u001b[0m                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m202 \u001b[2m│   \u001b[0mmodel, _, _ = load_model(config, opt.ckpt_path,                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0minject_lora=opt.inject_lora,              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_scale=opt.lora_scale,                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m205 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_path=opt.lora_path                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/../scripts/\u001b[0m\u001b[1;33msampl\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33me_utils.py\u001b[0m:\u001b[94m14\u001b[0m in \u001b[92mload_model\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 11 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading model from \u001b[0m\u001b[33m{\u001b[0mckpt_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 12 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# load sd\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 14 \u001b[2m│   \u001b[0mpl_sd = torch.load(ckpt_path, map_location=\u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m)                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[2m│   │   \u001b[0mglobal_step = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mglobal_step\u001b[0m\u001b[33m\"\u001b[0m]                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m\u001b[2m│   │   \u001b[0mepoch = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mepoch\u001b[0m\u001b[33m\"\u001b[0m]                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m713\u001b[0m in \u001b[92mload\u001b[0m    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 710 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mopened_file.seek(orig_position)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.jit.load(opened_file)                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, pickle_mod \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 713 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _legacy_load(opened_file, map_location, pickle_module, \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 714 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 715 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m# Register pickling support for layout instances such as\u001b[0m              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m920\u001b[0m in         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[92m_legacy_load\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 917 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mReceived object of type \u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(f)\u001b[33m}\u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m. Please update to\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 918 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mfunctionality.\u001b[0m\u001b[33m\"\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 919 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 920 \u001b[2m│   \u001b[0mmagic_number = pickle_module.load(f, **pickle_load_args)          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 921 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m magic_number != MAGIC_NUMBER:                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInvalid magic number; corrupt file?\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   \u001b[0mprotocol_version = pickle_module.load(f, **pickle_load_args)      \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mUnpicklingError: \u001b[0minvalid load key, \u001b[32m'v'\u001b[0m.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-20 17:00:52.862052: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-20 17:00:52.897046: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-20 17:00:53.425782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Global seed set to 1000\n",
      "config: \n",
      " {'model': {'target': 'lvdm.models.ddpm3d.LatentDiffusion', 'params': {'linear_start': 0.00085, 'linear_end': 0.012, 'num_timesteps_cond': 1, 'log_every_t': 200, 'timesteps': 1000, 'first_stage_key': 'video', 'cond_stage_key': 'caption', 'image_size': [32, 32], 'video_length': 16, 'channels': 4, 'cond_stage_trainable': False, 'conditioning_key': 'crossattn', 'scale_by_std': False, 'scale_factor': 0.18215, 'unet_config': {'target': 'lvdm.models.modules.openaimodel3d.UNetModel', 'params': {'image_size': 32, 'in_channels': 4, 'out_channels': 4, 'model_channels': 320, 'attention_resolutions': [4, 2, 1], 'num_res_blocks': 2, 'channel_mult': [1, 2, 4, 4], 'num_heads': 8, 'transformer_depth': 1, 'context_dim': 768, 'use_checkpoint': True, 'legacy': False, 'kernel_size_t': 1, 'padding_t': 0, 'temporal_length': 16, 'use_relative_position': True}}, 'first_stage_config': {'target': 'lvdm.models.autoencoder.AutoencoderKL', 'params': {'embed_dim': 4, 'monitor': 'val/rec_loss', 'ddconfig': {'double_z': True, 'z_channels': 4, 'resolution': 256, 'in_channels': 3, 'out_ch': 3, 'ch': 128, 'ch_mult': [1, 2, 4, 4], 'num_res_blocks': 2, 'attn_resolutions': [], 'dropout': 0.0}, 'lossconfig': {'target': 'torch.nn.Identity'}}}, 'cond_stage_config': {'target': 'lvdm.models.modules.condition_modules.FrozenCLIPEmbedder'}}}}\n",
      "Loading model from models/base_t2v/model.ckpt\n",
      "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m263\u001b[0m in \u001b[92m<module>\u001b[0m                                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m260 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m261 \u001b[0m\u001b[2m# --------------------------------------------------------------------\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m262 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m263 \u001b[2m│   \u001b[0mmain()                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m264 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/\u001b[0m\u001b[1;33msample_text2vide\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33mo.py\u001b[0m:\u001b[94m202\u001b[0m in \u001b[92mmain\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m199 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mconfig: \u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m, config)                                        \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m200 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m201 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# get model & sampler\u001b[0m                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m202 \u001b[2m│   \u001b[0mmodel, _, _ = load_model(config, opt.ckpt_path,                    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m203 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0minject_lora=opt.inject_lora,              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m204 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_scale=opt.lora_scale,                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m205 \u001b[0m\u001b[2m│   │   │   │   │   │   │    \u001b[0mlora_path=opt.lora_path                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/src/notebooks/T2V_models/VideoCrafter/VideoCrafter/scripts/../scripts/\u001b[0m\u001b[1;33msampl\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[1;33me_utils.py\u001b[0m:\u001b[94m14\u001b[0m in \u001b[92mload_model\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 11 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mLoading model from \u001b[0m\u001b[33m{\u001b[0mckpt_path\u001b[33m}\u001b[0m\u001b[33m\"\u001b[0m)                           \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 12 \u001b[0m\u001b[2m│   \u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 13 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# load sd\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 14 \u001b[2m│   \u001b[0mpl_sd = torch.load(ckpt_path, map_location=\u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m)                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 15 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mtry\u001b[0m:                                                               \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 16 \u001b[0m\u001b[2m│   │   \u001b[0mglobal_step = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mglobal_step\u001b[0m\u001b[33m\"\u001b[0m]                             \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 17 \u001b[0m\u001b[2m│   │   \u001b[0mepoch = pl_sd[\u001b[33m\"\u001b[0m\u001b[33mepoch\u001b[0m\u001b[33m\"\u001b[0m]                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m713\u001b[0m in \u001b[92mload\u001b[0m    \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 710 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mopened_file.seek(orig_position)                   \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 711 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.jit.load(opened_file)                \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 712 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, pickle_mod \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 713 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m _legacy_load(opened_file, map_location, pickle_module, \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 714 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 715 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 716 \u001b[0m\u001b[2m# Register pickling support for layout instances such as\u001b[0m              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.8/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m920\u001b[0m in         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[92m_legacy_load\u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 917 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mReceived object of type \u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(f)\u001b[33m}\u001b[0m\u001b[33m\\\"\u001b[0m\u001b[33m. Please update to\u001b[0m \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 918 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mfunctionality.\u001b[0m\u001b[33m\"\u001b[0m)                                         \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 919 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 920 \u001b[2m│   \u001b[0mmagic_number = pickle_module.load(f, **pickle_load_args)          \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 921 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m magic_number != MAGIC_NUMBER:                                  \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 922 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mInvalid magic number; corrupt file?\u001b[0m\u001b[33m\"\u001b[0m)     \u001b[31m│\u001b[0m\n",
      "\u001b[31m│\u001b[0m   \u001b[2m 923 \u001b[0m\u001b[2m│   \u001b[0mprotocol_version = pickle_module.load(f, **pickle_load_args)      \u001b[31m│\u001b[0m\n",
      "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
      "\u001b[1;91mUnpicklingError: \u001b[0minvalid load key, \u001b[32m'v'\u001b[0m.\n"
     ]
    }
   ],
   "source": [
    "OUTDIR=\"../../generated_videos/VideoCrafter\"\n",
    "\n",
    "for PROMPT in text_prompts:\n",
    "    ! python3 scripts/sample_text2video.py \\\n",
    "        --ckpt_path $BASE_PATH \\\n",
    "        --config_path $CONFIG_PATH \\\n",
    "        --prompt \"$PROMPT\" \\\n",
    "        --save_dir $OUTDIR \\\n",
    "        --n_samples 1 \\\n",
    "        --batch_size 1 \\\n",
    "        --seed 1000 \\\n",
    "        --show_denoising_progress"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "VideoCrafter",
   "language": "python",
   "name": "videocrafter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
